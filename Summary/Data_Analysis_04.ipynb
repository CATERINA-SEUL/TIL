{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Data_Analysis_04.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsmTbkOuB5hf",
        "colab_type": "text"
      },
      "source": [
        "### Deep learning \n",
        "\n",
        "   1.\tperceptron\n",
        "    -\t인공신경망의 기본 단위 \n",
        "    이전 뉴런과 연결  수상돌기  축삭돌기  다음 뉴런 \n",
        "    \n",
        "    $\\rightarrow$ inputs  weights (연결강도를 표현)  \n",
        "        net input function  activation function\n",
        "\n",
        "    •\tartificial neural network (ANN) == perception 을 여러 개 연결한 것 \n",
        "\n",
        "   2.\tdeep learning\n",
        "    -\tsingle layer perceptron & multi layer perceptron \n",
        "    * hidden layer가 1개 이상 있으면 multi\n",
        "\n",
        "    * Deep neural network 를 이용한 machine learning 방법\n",
        "        == hidden layer 수가 2개 이상인 network\n",
        "\n",
        "   3.\tgradient descent\n",
        "    -\t미분… ㅎ (접선의 기울기) \n",
        "    -\t원하는 weight 를 어떻게 찾을까?\n",
        "        아무것도 모르니 random 한 값으로 시작하자\n",
        "    -\tRandom Initialization ! \n",
        "\n",
        "---\n",
        "#### Loss Function (cost function)\n",
        "\n",
        "   * 예측한 데이터의 차이.\n",
        "       \n",
        "       - Loss Function 의 값이 줄어들도록 weight 값을 정리. \n",
        "       - 미분 값의 반대방향으로 가는 것 !!!! (조금씩 w 를 바꿔 LOSS 감소)\n",
        "\n",
        "   * Recap-gradient Descent\n",
        "     \n",
        "     - 모든 training data 에 대하여 neural network 의 출력과 실제 정답을 비교하여 \n",
        "       각각의 LOSS를 계산하고 이를 모두 더해서 전체 LOSS를 계산한다. \n",
        "   * 전체 loss 를 weight 로 미분  그 gradient 가 가리키는 방향의 반대로 조금씩 바꿈 \n",
        "   * data 가 아주 많을 때는 weight 값을 아주 조금씩 바꾸기 위해서 모든 data의 loss를 \n",
        "     다 계산해야함…\n",
        "---\n",
        "\n",
        "   4.\tmini-batch 학습법\n",
        "    -\tbatch gradient descent == 모든 data의 loss 계산\n",
        "    -\tstochastic gradient descent == 1개의 data 에 대한 loss 계산\n",
        "    -\tmini-batch gradient descent == data를 n개 뽑고 그 n개의 loss 값을 더해서 이용\n",
        "\n",
        "   5.\tback propagation\n",
        "    -\t미분 값을 반대방향으로 계산한 후에 다 곱함!!! (Chain role)\n",
        "\n",
        "   6.\toverfitting\n",
        "    -\tdata set  나누기(2, 3가지로 나눔)\n",
        "    -\ttraining set : 학습에 사용하는 data\n",
        "    -\tvalidation set : 학습에 사용하지 않고 hyper parameter tuning 에 사용하는 data\n",
        "    -\ttest set : model 평가하기 위한 data \n",
        "\n",
        "---\n",
        "### Deep learning 목적\n",
        "  - 학습에 사용하지 않았던 미지의 data가 들어왔을 때, 예측. \n",
        "\n",
        "###### Test error == training error + generalization gap\n",
        " - Model capacity 가 작으면 성능이 안 나오고 너무 크면 generalization gap이 커짐 \n",
        "                    == underfitting \t\t== overfitting(융통성이 없어)\n",
        "\n",
        "----\n",
        "##### overfitting 을 막는 방법 \n",
        "\n",
        "    1)\tData 양을 늘린다.\n",
        "    2)\tRegularization 방법을 사용한다. \n",
        "\n",
        "   7.\tconvolutional neural network\n",
        "    -\tCNN\n",
        "    == 이미지 인식에 가장 널리 사용됨\n",
        "     일반적으로 convolution layer – feature 추출\n",
        "     pooling layer – 추출된 feature 모음\n",
        "     fully-connected layer – 최종적으로 모인 feature 을 가지고 class 판단\n",
        "\n",
        "    * 동작원리\n",
        "    이미지를 작은 타일로 나누고 첫번째 타일에서 특정 feature 추출\n",
        "     하나씩 이동하면서 feature 추출(반복)\n",
        "\n",
        "   8.\trecurrent neural network\n",
        "    -\tRNN\n",
        "    -\tZ  A 로 외우는 건 어렵… \n",
        "    -\t이전까지 들어온 입력이 저장된 공간 – state가 존재\n",
        "    -\tsequence가 길어지면 성능이 떨어진다, 학습도 안됨, 기억을 잘 못한다.\n",
        "\n",
        "    * RNN의 문제점을 해결하기 위해 만들어진 Network들\n",
        "    -\tLSTM – long short term memory \n",
        "    -\tGRU – gated recurrent unit\n",
        "\n",
        "   9.\tgenerative adversarial network\n",
        "    -\tGAN\n",
        "    -\t생성 모델의 한 종류로 서로 대립하는 두 신경망을 경쟁시켜 좋은 성능을 이끌어내는 딥러닝 알고리즘\n",
        "\n",
        "    10.\ttensorflow & pytorch\n",
        "    -\ttensorflow : python 을 사용하는 오픈 소프트웨어\n",
        "    -\tpytorch : python 후속제품\n",
        "\n",
        "\n",
        "---\n",
        "### 비교\n",
        "##### 가장 큰 차이점 !!\n",
        "1. tensorflow : static – define and run\n",
        "\t\t\n",
        "        - 디버깅이 어려움\n",
        "\t\t- 강력한 시각화 툴\n",
        "\t\t- rich community\n",
        "\t\t- 상업용, 연구용\n",
        "\t\t- 모바일용 라이브러리를 따로 가지고 있음\n",
        "        \n",
        "2. pytorch : dynamic – define by run (직관적)\n",
        "\t\t\n",
        "        - 디버깅이 쉬움\n",
        "\t\t- 연구용으로 많이 사용\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l_XAq4sB5hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}